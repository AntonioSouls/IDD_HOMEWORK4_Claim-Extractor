{
    "table": [
        [
            "Row",
            "Method",
            "ADD(-S)"
        ],
        [
            "A0",
            "GDR-Net baseline",
            "59.29"
        ],
        [
            "A1",
            "GDR-Net\u00a0[46]",
            "62.2"
        ],
        [
            "B0",
            "A0 + 3D LC loss",
            "66.48"
        ],
        [
            "B1",
            "A0 + 2D LC loss",
            "65.99"
        ],
        [
            "C0",
            "B0 + detach residual from E_{cov}",
            "61.03"
        ],
        [
            "C1",
            "B0 + detach weights from E_{cov}",
            "65.23"
        ],
        [
            "C2",
            "B0 + remove E_{cov}",
            "45.61"
        ],
        [
            "C3",
            "B0 + remove E_{linear}",
            "65.82"
        ],
        [
            "C4",
            "B0 + remove E_{prior}",
            "60.69"
        ]
    ],
    "caption": "Table 4: Ablation study of the dense correspondence-based method on LM-O. Block\u00a0A: Comparison of the baseline method and the original method. Block\u00a0B: Comparison between losses derived from different pose representations. Block\u00a0C: Ablations of each component of the linear-covariance loss on the dense correspondence-based baseline.",
    "references": [
        "We evaluate the influence of each component of our LC loss by applying it to the dense correspondence-based GDR-Net and to the sparse correspondence-based one on the LM-O dataset.\r\nThe results are summarized in Tab.\u00a04 and Tab.\u00a05.",
        "Effectiveness of the Linear Loss.\u2003The linear loss is a linear approximation of the actual pose error. As it acts on the weights, it seeks to emphasizes the correspondences that are more beneficial for the pose. As shown in Tab.\u00a04, after removing El\u200bi\u200bn\u200be\u200ba\u200brsubscript\ud835\udc38\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc4e\ud835\udc5fE_{linear} from the LC loss, the ADD(-S) score drops slightly (B0\u00a0vs.\u00a0C3). However, this makes a significant difference on the learned coordinate and weight maps; as illustrated in Fig.\u00a06\u00a0(c), when trained without linear loss, the network tends to confidently extrapolate the 3D coordinates to pixels far away from the target region or occluded. Such extrapolated coordinates are unreliable compared to correspondences near or inside object region, and thus they are suppressed by the linear loss."
    ]
}